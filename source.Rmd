---
title: "Practical Machine Learning - Peer Assessments Report"
author: "Thuan Bui"
date: "Monday, February 16, 2015"
output: html_document
---

```{r loading_libraries, cache=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
library(caret)
library(car)
library(nnet)
```

###This report describle how the model is built, steps used in cross validation, what the expected out of sample error is, and steps to build up final model.

# Preprocessing data and spliting into training and testing

```{r pre_processing_data, cache=FALSE, message=FALSE, warning=FALSE, echo=TRUE}
trainingData <- read.csv("data/train/pml-training.csv", na.strings=c("","NA", "#DIV/0!"))
trainingData <- subset(trainingData, select = -c(X, user_name, cvtd_timestamp))
trainingData$classe <- factor(trainingData$classe)
trainingData[is.na(trainingData)] = 0

testingData <- read.csv("data/test/pml-testing.csv", na.strings=c("","NA", "#DIV/0!"))
testingData <- subset(testingData, select = -c(X, user_name, cvtd_timestamp))
testingData[is.na(testingData)] = 0
```

# Fit model

### 1. Using cross validation to fit model

**Step 1** Use K-folds with k = 5 for resampling data.

**Step 2** Fit a multinom logistic regression model.

**Step 3** Repeat `Step 1` three times.

**Step 4** Aggregate the fitted models into final model.

```{r cross_validation, cache=FALSE, message=FALSE, warning=FALSE, echo=TRUE, include=FALSE}
# Prepare training and testing data
inTrain <- createDataPartition(trainingData$classe, p = .7, list = FALSE)
subTrainingData <- trainingData[inTrain, ]
subTestingData <- trainingData[-inTrain, ]

# Fit model
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 3)

set.seed(78)
modelFit <- train(classe ~ ., data = subTrainingData,
                 method = "multinom",
                 trControl = fitControl,
                 preProc = c("center", "scale"))
```

**I use the `train` method of Caret package with `multinom` method to fit model following 4 steps above.**

```{r cross_validation_source, cache=FALSE, message=FALSE, warning=FALSE, echo=TRUE, eval=FALSE}
# Prepare training and testing data
inTrain <- createDataPartition(trainingData$classe, p = .7, list = FALSE)
subTrainingData <- trainingData[inTrain, ]
subTestingData <- trainingData[-inTrain, ]

# Fit model
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 3)

set.seed(78)
modelFit <- train(classe ~ ., data = subTrainingData,
                 method = "multinom",
                 trControl = fitControl,
                 preProc = c("center", "scale"))
```

### 2. Final model result

```{r cross_validation_print, cache=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
print(modelFit)
```

# Out of sample error

The expected out of sample error is the error rate of prediction base on testing data. I used `confusionMatrix` of `caret` packge to estimate out of sample error with cross validation.

### Do prediction for testing data

```{r predictions, cache=FALSE, message=FALSE, warning=FALSE, echo=TRUE}
predictions <- predict(modelFit, newdata = subTestingData)
print(confusionMatrix(predictions, subTestingData$classe))
```

# Prediction Assignment Submission

### Do prediction for testing data

```{r predictions_for_submission, cache=FALSE, message=FALSE, warning=FALSE, echo=TRUE}
predictionsForSubmitting <- predict(modelFit, newdata = testingData)
print(predictionsForSubmitting)
```

### Save answer to text files
```{r submit_answer, cache=FALSE, message=FALSE, warning=FALSE, echo=TRUE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictionsForSubmitting)
```

